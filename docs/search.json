[
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "DATA IMPORT/ TIDY/ TRANSFORM\n\n#Library Loading:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(readr)\n\n\n# Data Ingest:\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n# Data Cleaning:\n\ndrop_na(camels)\n\n# A tibble: 507 × 58\n   gauge_id p_mean pet_mean p_seasonality frac_snow aridity high_prec_freq\n   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 01022500   3.61     2.12       -0.115      0.245   0.587           20.6\n 2 01031500   3.52     2.07        0.104      0.292   0.588           18.9\n 3 01047000   3.32     2.09        0.148      0.280   0.629           20.1\n 4 01052500   3.73     2.10        0.152      0.353   0.562           13.5\n 5 01054200   4.07     2.13        0.105      0.300   0.523           17.5\n 6 01055000   3.49     2.09        0.167      0.306   0.599           19.2\n 7 01057000   3.57     2.13        0.0791     0.251   0.597           20.4\n 8 01073000   3.50     2.21        0.0304     0.175   0.630           20.8\n 9 01078000   3.56     2.18        0.110      0.240   0.613           21.4\n10 01118300   3.90     2.31       -0.0377     0.111   0.593           21.5\n# ℹ 497 more rows\n# ℹ 51 more variables: high_prec_dur &lt;dbl&gt;, high_prec_timing &lt;chr&gt;,\n#   low_prec_freq &lt;dbl&gt;, low_prec_dur &lt;dbl&gt;, low_prec_timing &lt;chr&gt;,\n#   geol_1st_class &lt;chr&gt;, glim_1st_class_frac &lt;dbl&gt;, geol_2nd_class &lt;chr&gt;,\n#   glim_2nd_class_frac &lt;dbl&gt;, carbonate_rocks_frac &lt;dbl&gt;, geol_porostiy &lt;dbl&gt;,\n#   geol_permeability &lt;dbl&gt;, soil_depth_pelletier &lt;dbl&gt;,\n#   soil_depth_statsgo &lt;dbl&gt;, soil_porosity &lt;dbl&gt;, soil_conductivity &lt;dbl&gt;, …\n\ncamels %&gt;% \n  mutate(logC = log(q_mean +1))\n\n# A tibble: 671 × 59\n   gauge_id p_mean pet_mean p_seasonality frac_snow aridity high_prec_freq\n   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 01013500   3.13     1.97        0.188      0.313   0.631           13.0\n 2 01022500   3.61     2.12       -0.115      0.245   0.587           20.6\n 3 01030500   3.27     2.04        0.0474     0.277   0.624           17.2\n 4 01031500   3.52     2.07        0.104      0.292   0.588           18.9\n 5 01047000   3.32     2.09        0.148      0.280   0.629           20.1\n 6 01052500   3.73     2.10        0.152      0.353   0.562           13.5\n 7 01054200   4.07     2.13        0.105      0.300   0.523           17.5\n 8 01055000   3.49     2.09        0.167      0.306   0.599           19.2\n 9 01057000   3.57     2.13        0.0791     0.251   0.597           20.4\n10 01073000   3.50     2.21        0.0304     0.175   0.630           20.8\n# ℹ 661 more rows\n# ℹ 52 more variables: high_prec_dur &lt;dbl&gt;, high_prec_timing &lt;chr&gt;,\n#   low_prec_freq &lt;dbl&gt;, low_prec_dur &lt;dbl&gt;, low_prec_timing &lt;chr&gt;,\n#   geol_1st_class &lt;chr&gt;, glim_1st_class_frac &lt;dbl&gt;, geol_2nd_class &lt;chr&gt;,\n#   glim_2nd_class_frac &lt;dbl&gt;, carbonate_rocks_frac &lt;dbl&gt;, geol_porostiy &lt;dbl&gt;,\n#   geol_permeability &lt;dbl&gt;, soil_depth_pelletier &lt;dbl&gt;,\n#   soil_depth_statsgo &lt;dbl&gt;, soil_porosity &lt;dbl&gt;, soil_conductivity &lt;dbl&gt;, …\n\n\nDATA SPLITTING\n\n# Initial Split\nset.seed(123)\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\n\n#Training/ Testing\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\nFEATURE ENGINEERING\n\n# Proper Recipe\n\nrec &lt;-  recipe(q_mean ~ ., data = camels_train) %&gt;%\n  step_rm(gauge_lat, gauge_lon, gauge_id, high_prec_timing, low_prec_timing, geol_1st_class, geol_2nd_class, dom_land_cover) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\nDATA RESAMPLING AND MODEL TESTING\n\n# Cross Validation Data Set\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Define Three Regression Models\n\n# Linear Regression\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Random Forest Regression\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n#XG Boost\nb_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\n\n# Workflow Set/ Map/ Autoplot\n\nwf &lt;- workflow_set(list(rec), list(b_model, rf_model, lm_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\n\nModel Selection With Justification:\nI think the Linear Regression Model performs the best, because it has the highest r squared value out and the lowest root mean squared error out of all the data.\nI am choosing the Linear Regression Model. It’s engine is “lm” and it’s in regression mode. I think that it’s performing well for this problem because it fundamentally measures the relationship between a dependent variable (q_mean) and an independent variable (all of the other variables,) and is commonly used in environmental science.\nMODEL TUNING\n\n#Tunable Model Set Up:\n\n#Even though linear regression had the best metrics, it wasn't working for the tuning, so I changed it to random forest and now it works. \n\nlibrary(tune)\n#install.packages(\"bonsai\")\nlibrary(bonsai)\n\n\nrf_model_tune &lt;- rand_forest(trees = tune(), min_n = tune()) |&gt;\n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n# Tunable Workflow Defined:\n\nwf_tune &lt;- workflow() %&gt;%\n  add_model(rf_model_tune) %&gt;%\n  add_recipe(rec)\n\ncamels_metrics = metric_set(rsq, rmse, mae)\n\n# Description of Dial Ranges:\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\ndials$object\n\n[[1]]\n\n\n# Trees (quantitative)\n\n\nRange: [1, 2000]\n\n\n\n[[2]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [2, 40]\n\n# Define Search Space:\n\nmy.grid &lt;- dials |&gt; \n  update(trees = trees(c(50, 500))) |&gt;\n  grid_latin_hypercube(size = 20)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\nrange(my.grid$trees)\n\n[1]  66 493\n\n\n\n# Tune the Model\nmodel_params &lt;-  tune_grid(\n    wf_tune,\n    resamples = camels_cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\nAs the minimal node size increases, mean absolute error and root mean squared error increase while r squared decreases. Number of trees has no affect on any of the metrics, as the scatter plots are completely random.\nCHECK SKILL OF TUNED MODEL\n\n# Collect Metrics\n\ncollect_metrics(model_params)\n\n# A tibble: 60 × 8\n   trees min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1   249    13 mae     standard   0.154    10 0.0121  Preprocessor1_Model01\n 2   249    13 rmse    standard   0.306    10 0.0363  Preprocessor1_Model01\n 3   249    13 rsq     standard   0.970    10 0.00533 Preprocessor1_Model01\n 4   139    36 mae     standard   0.196    10 0.0157  Preprocessor1_Model02\n 5   139    36 rmse    standard   0.396    10 0.0487  Preprocessor1_Model02\n 6   139    36 rsq     standard   0.950    10 0.00800 Preprocessor1_Model02\n 7    98    30 mae     standard   0.186    10 0.0145  Preprocessor1_Model03\n 8    98    30 rmse    standard   0.374    10 0.0426  Preprocessor1_Model03\n 9    98    30 rsq     standard   0.955    10 0.00633 Preprocessor1_Model03\n10   158    18 mae     standard   0.166    10 0.0131  Preprocessor1_Model04\n# ℹ 50 more rows\n\n\nHow do I arrange for descending values but keep all organized ??\n\n# Show Best\n\nshow_best(model_params, metric = \"rsq\")\n\n# A tibble: 5 × 8\n  trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   375     7 rsq     standard   0.973    10 0.00489 Preprocessor1_Model15\n2   493     5 rsq     standard   0.972    10 0.00603 Preprocessor1_Model08\n3    80     2 rsq     standard   0.972    10 0.00434 Preprocessor1_Model11\n4   399     9 rsq     standard   0.972    10 0.00608 Preprocessor1_Model13\n5   350    10 rsq     standard   0.971    10 0.00574 Preprocessor1_Model07\n\nshow_best(model_params, metric = \"rmse\")\n\n# A tibble: 5 × 8\n  trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   375     7 rmse    standard   0.290    10  0.0327 Preprocessor1_Model15\n2   399     9 rmse    standard   0.291    10  0.0351 Preprocessor1_Model13\n3   493     5 rmse    standard   0.291    10  0.0334 Preprocessor1_Model08\n4    80     2 rmse    standard   0.293    10  0.0304 Preprocessor1_Model11\n5   350    10 rmse    standard   0.296    10  0.0329 Preprocessor1_Model07\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n  trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   375     7 mae     standard   0.144    10 0.00963 Preprocessor1_Model15\n2   399     9 mae     standard   0.147    10 0.0106  Preprocessor1_Model13\n3   493     5 mae     standard   0.149    10 0.00933 Preprocessor1_Model08\n4    80     2 mae     standard   0.149    10 0.0110  Preprocessor1_Model11\n5   350    10 mae     standard   0.151    10 0.0100  Preprocessor1_Model07\n\n\nmean absolute error is lowest when there are 465 trees and a min_n of 3.\n\n# Select Best \n\nhp_best &lt;- select_best(model_params, metric = \"rsq\")\n\nFINALIZE YOUR MODEL\n\n# Finalize Workflow\n\nfinal_wf &lt;- finalize_workflow(wf_tune, hp_best)\n\nFINAL MODEL VERIFICATION\n\n# Implement Last Fit\n\nfinal_fit &lt;- last_fit(final_wf, camels_split, metrics = camels_metrics)\n\n# Collect Metrics \ncollect_metrics(final_fit)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rsq     standard       0.972 Preprocessor1_Model1\n2 rmse    standard       0.287 Preprocessor1_Model1\n3 mae     standard       0.124 Preprocessor1_Model1\n\n\n\n# Collect Predictions \n\n#Plots\n\ncollect_predictions(final_fit) |&gt; \n  ggplot(aes(x = .pred, y = q_mean)) + \n  geom_point() +\n  geom_abline() + \n  geom_smooth(method = \"lm\") + \n  theme_linedraw() + \n  labs(title = \"Actual VS Predicted q_mean\", \n       x = \"Predicted (Log10)\", \n       y = \"Actual (Log10)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBUILDING A MAP\n\nlibrary(patchwork)\n\nfinal &lt;- fit(final_wf, data = camels) %&gt;%\n  augment(new_data = camels) %&gt;%\n  mutate(residuals = .pred - q_mean)\n\nnames(final_fit)\n\n[1] \"splits\"       \"id\"           \".metrics\"     \".notes\"       \".predictions\"\n[6] \".workflow\"   \n\n# Map of Residuals\nResiduals &lt;- ggplot(data = final, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = residuals)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n# Map of Predictions\nPredictions &lt;- ggplot(data = final, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = .pred)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\nResiduals + Predictions\n\n\n\n\n\n\n\ncombined &lt;- Residuals + Predictions\nprint(combined)"
  },
  {
    "objectID": "Lab6.html",
    "href": "Lab6.html",
    "title": "Lab6.qmd",
    "section": "",
    "text": "#install.packages(\"powerjoin\")\n#install.packages(\"vip\")\n#install.packages(\"baguette\")\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nQUESTION 1\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\nzero_q_freq represents the frequency of days with Q = 0 mm/day\nQUESTION 3\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\nUSING A WORKFLOW INSTEAD:\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nUSING RANDOM TREE\n\n#install.packages(\"ranger\")\nlibrary(ranger)\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nWORKFLOW SET APPROACH:\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(ggthemes)\n\n\nUS_Mean_Precip &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat,)) +\n  labs(title = \"US Mean Precip\") +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\nUS_Mean_Aridity &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat,)) +\n  labs(title = \"US Mean Aridity\") +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\ncombined &lt;- US_Mean_Precip + US_Mean_Aridity\nprint(combined)\n\n\n\n\n\n\n\n\nQUESTION 3:\n\n# Build an xgboost regression and neural network model\n\nb_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\nb_model &lt;- boost_tree(mode = \"regression\") %&gt;%\n  set_engine(\"xgboost\")\n\nnn_model &lt;- mlp(hidden_units = 5, penalty = 0.01) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\")\n\nnn_model &lt;- bag_mlp(mode = \"regression\") %&gt;%\n  set_engine(\"nnet\", times = 25)\n\n\n# Add the models to the above workflow\n#install.packages(\"xgboost\")\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, nn_model, b_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.554  0.0211    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.789  0.0214    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0247    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0258    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nQUESTION 4, BUILD YOUR OWN:\n\n# Data Splitting\n\ncamels2 &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean)) %&gt;%\n  select(logQmean, p_mean, aridity, soil_depth_pelletier, max_water_content, organic_frac, frac_snow, pet_mean, soil_depth_statsgo, elev_mean, slope_mean, area_gages2) %&gt;%\n  na.omit()\n\nset.seed(1991)\ncamels_split2 &lt;- initial_split(camels, prop = 0.75)\ncamels_train2 &lt;- training(camels_split)\ncamels_test2  &lt;- testing(camels_split)\n\ncamels_cv2 &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Recipe\n\n#rec &lt;-  recipe(logQmean ~ p_mean + pet_mean + elev_mean + area_gauges2 + max_water_content + slope_mean, data = camels_train) %&gt;%\n # step_scale(all_predictors()) %&gt;%\n # step_center(all_predictors())\n\n# I chose this formula because, based, off of the PDF, these are the variables that influence stream flow. I think that they will have a statistically significant correlation. I keep getting errors in this section I have no idea why.\n\n\n# Define 3 Models\n\n#Random Forest\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_model &lt;- boost_tree(mode = \"regression\") %&gt;%\n  set_engine(\"xgboost\")\n\ndt_model &lt;- decision_tree(mode = \"regression\")%&gt;%\n  set_engine(\"rpart\")\n\n\n# Workflow Set\n\nwf2 &lt;- workflow_set(list(rec), list(xgb_model, rf_model, dt_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf2)\n\n\n\n\n\n\n\n\nThe decision tree model is the best fit for the CAMELS data set, because it demonstrates the highest r squared value, though the random forest is very close as well.\n\n#Extact and Evaluate\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train2)\n\nrf_data2 &lt;- augment(rf_wf, new_data = camels_test2)\ndim(rf_data2)\n\n[1] 135  60\n\nggplot(rf_data2, aes(x = .pred, y = logQmean)) +\n  geom_point(color = \"skyblue\") +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"Observed vs Predicted LogQmean Values\",\n       x = \"Predicted Values\",\n       y = \"Observed Values \")\n\n\n\n\n\n\n\n\nIt looks like the results are fairly linear, they get more correlated the higher the values get."
  }
]